\section{Introduction}
The ineffectiveness of existing regular expression DPI methods against encrypted payloads and weakly signatured data classes as well as the computational and storage complexity of automata has caused researchers to shift to other forms of traffic fingerprinting for identification. We propose using locality-sensitive hashing in order to measure similarity of features between packets. Locality-sensitive hashing uses a distance metric to preserve the similarity of original inputs when they are converted to locality-sensitive hashes (LSH). The benefit of locality-sensitive hashing here is that it is capable of reducing high-dimensionality data into a low-dimensional, space-effective, computationally efficient representation of the same data while still preserving similarity metrics between the data points~\cite{jafari2021survey}. Hash families may be defined for many distance measures such as Hamming distance, L1 and L2 norm~\cite{ANN-CoD}, and Jaccard similarity~\cite{minhash}.

Contrary to cryptographic hashes which attempt to avoid collision, locality-sensitive hashes preserve the similarity of data points (in our case, token sets) to one another. The following definition intuitively states that data points which are locally nearby have a higher probability of collision than further points.

\medskip

\begin{definition}
\textit{~\cite{lshforest} A family $H$ of functions from a domain $S$ to a range $U$ is called ($r$, $\epsilon$, $p_1$, $p_2$)-sensitive, with $r$, $\epsilon > 0$, $p_1 > 0$, $p_2 > 0$, if for any $p$, $q \in S$, the following conditions hold:}
\begin{itemize}
\item{\textit{if $D(p, q) \leq r$ then $Pr_H[h(p) = h(q)] \geq p_1$,}}
\item{\textit{if $D(p, q) > r(1 + \epsilon)$ then $Pr_H[h(p) = h(q)] \leq p_2$.}}
\end{itemize}
\end{definition}

\medskip


One way in which locality-sensitive hashing has been applied in the network security domain is in user-level browser fingerprinting. In this method, web-based browser fingerprints created by extracting multiple values from the browser API may be hashed using MinHash or similar functions to generate signatures of high entropy, where the data is uniquely identifiable as a particular device or host~\cite{browser}. We apply this concept similarly to network packet data, aiming for a locality-sensitive hash using features which should also evidence values unique to the class type. LSH clustering has also been used to create signatures for identifying malware at scale~\cite{bayer}, but has rarely been applied in DPI.

Once hashes are created representative of the packets, differences in the hashes may be compared linearly using a similarity search approximation. Similarity search is a generalized term in data mining which refers to searching for objects where the available comparator is some common pattern or similarities among them. Examples of similarity search mechanisms include Nearest Neighbors searching, link-based similarity searches, duplicate detection, and defining object representations~\cite{lshforest}. Applications for this in the cyber crime space include finding individuals in the same criminal network, finding content online similar to known evidence, querying to find images similar to another, and detecting fingerprints across network data. The best indexes for similarity search have efficient querying and storage mechanisms, minimum memory footprint, and minimal interference required by the developer and maintainer~\cite{simsearch}.

One of the challenges in data engineering is determining how to construct suitable, unique \textit{tokens} which characterize the data meaningfully. The sentence, ``Palm trees are native to the Pacific", may be split on whitespace into the set of tokens $T = \{\text{``palm"}, \text{``trees"}, \text{``are"}, \text{``native"}, \text{``to"}, \text{``the"}, \text{``pacific"}\}$. It is obvious that tokens like \text{``the"} are far too generic in the English language to be characteristic of this sentence, but a token like \text{``palm"} or \text{``pacific"} will be much more unique and indicate a stronger similarity. This idea may be more solidly quantified through taking the inverse document frequency (IDF) of tokens which minimizes the importance of terms which appear frequently in the document set. When combined with term frequency (TF) as the TF-IDF value~\cite{tf-idf}, the most relevant tokens may be found which are both common and characteristically unique of the document set.
