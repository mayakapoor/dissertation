\section{Introduction}
The ineffectiveness of existing regular expression-based deep packet inspection methods against encrypted payloads and weakly signatured data classes as well as the computational and storage complexity of automata has caused researchers to shift to other forms of traffic fingerprinting for identification. We propose using locality-sensitive hashing in order to measure similarity of features between packets. Locality-sensitive hashing uses a distance metric to preserve the similarity of original inputs when they are converted to locality-sensitive hashes (LSH). The benefit of locality-sensitive hashing here is that it is capable of reducing high-dimensionality data into a low-dimensional, space-effective, computationally efficient representation of the same data while still preserving similarity metrics between the data points~\cite{jafari2021survey}. Hash families may be defined for many distance measures such as Hamming distance, L1 and L2 norm~\cite{ANN-CoD}, and Jaccard similarity~\cite{minhash}.

While cryptographic hashes attempt to minimize collisions, locality-sensitive hashes preserve the similarity of data points by generating hashes closer to one another depending on the input's similarity. The following definition intuitively states that data points which are locally nearby have a higher probability of collision than further points.

\medskip

\begin{definition}
\textit{~\cite{lshforest} A family $H$ of functions from a domain $S$ to a range $U$ is called ($r$, $\epsilon$, $p_1$, $p_2$)-sensitive, with $r$, $\epsilon > 0$, $p_1 > 0$, $p_2 > 0$, if for any $p$, $q \in S$, the following conditions hold:}
\begin{itemize}
\item{\textit{if $D(p, q) \leq r$ then $Pr_H[h(p) = h(q)] \geq p_1$,}}
\item{\textit{if $D(p, q) > r(1 + \epsilon)$ then $Pr_H[h(p) = h(q)] \leq p_2$.}}
\end{itemize}
\end{definition}

\medskip

Locality-sensitive hashing has been used in user-level browser fingerprinting, where web-based browser fingerprints are created by extracting multiple values from the browser API and hashed using MinHash or some similar algorithm. This generates signatures of high entropy, where the data is uniquely identifiable as a particular device or host~\cite{browser}. LSH clustering has also been used to make signatures for classifying malware at scale~\cite{bayer}. In a similar fashion, we apply this technique to network packet data by using features which should evidence values unique to a given class and creating the locality-sensitive hash from those.

Similarity search approximation can be performed by linearly comparing the hashes created from the packets. Similarity search is a generalized term in data mining which refers to searching for objects where the available comparator is some common pattern or set of similarities. This group of mechanisms includes tools such as Nearest Neighbors searching, link-based similarity searches, duplicate detection, and object representation~\cite{lshforest}. This can be applied in cyber criminal investigation to find individuals in the same criminal network, content similar to known evidence, images similar to another, or known fingerprints across network data. Qualities of good similarity search methods include efficient querying and storage mechanisms, a minimized memory footprint, and minimal human interference~\cite{simsearch}.

One data engineering challenge is constructing suitable, unique \textit{tokens} which characterize the data meaningfully. The sentence, ``Palm trees are native to the Pacific'', may be split on whitespace into the set of tokens $T = \{\text{``palm''}, \text{``trees''}, \text{``are''}, \text{``native''}, \text{``to''}, \text{``the''}, \text{``pacific''}\}$. It is obvious that tokens like \text{``the''} are far too generic in the English language to be characteristic of this sentence, but a token like \text{``palm''} or \text{``pacific''} will be much more unique and indicate a stronger similarity. Formally, this idea is quantified as taking the inverse document frequency (IDF), where we minimize the importance of terms which appear frequently in the document set, and combining it with term frequency (TF) as the TF-IDF value~\cite{tf-idf}. This value describes tokens which are both common and characteristically unique of the document set.
