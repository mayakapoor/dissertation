\section{Discussion and Limitations}

Using both models in parallel, we were able to assess the effectiveness of multi-embeddings of both the \textsc{Alpine} and \textsc{Palm} models. The concatenation of multiple classifiers and agreement among voters in an ensemble has yielded a reduction in error and at best correlation when there is a misclassification~\cite{tumerensemble}. We hypothesized that enabling both \textsc{Alpine} and \textsc{Palm} to run together would yield the most accurate and agreed-upon classification results. In order to test this, we ran the 26-class protocol identification test against configurations of the model with just \textsc{Alpine} embedding, only \textsc{Palm} embedding, and a final combined run. Our results as shown in Table~\ref{table:embeddingresults} demonstrate that the combined votes of both models turned out to be the most accurate in the final tests. Furthermore, the agreeance among voters was also highest using multiple hash embeddings.

The locality-sensitive hashing approach is an improvement upon the regular expression approach to DPI in several ways. First, the hash generation process reduces the need for expert knowledge about a particular protocol. Previously, engineers would have to employ unique signatures for different message types. It would not be uncommon, for example, to have regular expression signatures for every method for a given text-based protocol like FTP or IMAP. Furthermore, requests and response messages often require unique signatures. Regexes are difficult for humans to read and comprehend. The management of server-client side interactions is also a challenge to know which versions to apply. Instead, hash embeddings using our forward-thinking system create a unique value for each provided input and indexes those values into one bucket. This even more generally captures the broad diversity of possible payload and header contents while providing a new layer of abstraction and simplicity. Flows may be examined bi-directionally, and all message types and methods considered. We are also not limited to traffic flows and protocol types. As demonstrated by the experiments in this paper, the system is capable of distinguishing more heterogenous or abstract class types like traffic type, mechanisms like encryption, and traditionally weak-signatured traffic like RTP. In terms of performance, the LSHForest as implemented in our model further reduces the storage and computational complexity when compared with a regular expression, automata-searching approach.

We posed the following research questions \textbf{R} to assess the \textsc{Alpine} and \textsc{Palm} models and report our findings \textbf{F} here:

\begin{itemize}
\item\textbf{R1.} Given labeled training data, can packets be classified by their application-layer protocol through the construction and index of locality-sensitive hashes?

\textbf{F1.} The results of our experiments show that for many application layer protocols, locality sensitive hashes may be constructed which are highly accurate for searching when indexed. Our model was 99.6\% accurate in its identifications across a wide scale of protocols. In the real world, this sort of expandable model has applications in DPI technology for traffic surveillance, quality of service management and network management, traffic profiling, and content and copyright enforcement. There are also additional filtering applications for cybersecurity and private network management. Detecting traffic at the protocol level remains an important task for network engineers, and our hashing technique improves upon the state of the art in both accuracy and breadth of scope.

\item\textbf{R2.} Can we also distinguish traffic types (such as email or VoIP data) from one another using this model? Does it generalize to other kinds of network traffic classes besides protocol?

\textbf{F2.} Our experiment in classifying packets by traffic type tests both an additional definition of class for network traffic as well as the ability of the model to handle more heterogeneous classes. Even in this problem the hash embedding technique proved highly accurate. This demonstrates the generalizability of the models to real data and new problems.

\item\textbf{R3.} For payload analysis, does using only high-value tokens as characterized by TF-IDF score improve distance measure results by isolating important features and reducing noise effectively in the data?

\textbf{F3.} We bring to the forefront our metric choice of Jaccard similarity because machine learning for network traffic analysis has trended toward more and more technically advanced and often computationally intensive techniques to improve model accuracy or desired metrics. The results of our experiments in this report are evidence that a more complex metric or model is not always the best direction. Even within subfields of machine learning like natural language processing, there is not a silver bullet solution which works best for all kinds of problems. While TF-IDF decently diminishes tokens such as ``a" and ``the" from the English language, the fact that ``HTTP\textbackslash1.1" appears in nearly every HTTP request in our dataset is actually relevant considering the context. Especially in network traffic analysis where scalability and line-rate capability is highly desirable, simple techniques can still be effective and should not be disregarded.

\item\textbf{R4.} Does the combination of the ALPINE and PALM models, i.e. multiple hash embeddings and concatenating those results, improve classification performance over any single hash embedding of a feature set?

\textbf{F4.} Multi-embeddings are a proposed solution particularly for more heterogeneous datasets. The idea is that capturing multiple angles or representations of the data and combining those results will yield a more full-scope picture and analysis. Regular expressions, on the other hand, are one-dimensional and incapable of such encoding without becoming increasingly complex. Furthermore, a single bad bit can cause a packet match to fail. Network traffic and packets in the wild are extremely diverse and can be prone to error, and thus are an ideal test set for the multi-embedding hash technique which is both more informative and resilient. The modular design of our system enables using one or more embeddings, which allows for a balance between performance impact and accuracy. Interestingly, though the \textsc{Palm} model was overall poorer-performing than the \textsc{Alpine} model, votes from the \textsc{Palm} model helped improve the combined result regardless.

\item\textbf{R5.} Does the number of classes, hashes, or sample size affect throughput or accuracy significantly?

\textbf{F5.} One concern for applied machine learning in traffic analysis and cybersecurity in general is the performance impact such techniques have. Thus, we provide transparent numbers for our system performance. All experiments in this trial were run on a 1.6 GHz Dual-Core Intel Core i5 processor with 16GB DDR3 memory. In future work, leveraging an accelerated processor such as an FPGA would greatly enhance performance and throughput due to the repetitive nature of hash computations. In profiling, the most impactful procedure in the model is MinHash, which is likely optimizable through an FPGA~\cite{fpga}. The measure of what is acceptable throughput depends on the network environment; however, in terms of scalability it is valuable that our model does not seem to increase exponentially or vary in performance wildly depending on the data. Rather, it is data independent. Our model also trains quickly, meaning that models could be swapped or trained online in the field if needed. Another potential avenue for future research is batch optimization and parallel processing. It would be beneficial and possible to run some of the training computations in parallel. Furthermore, the distance measures could be taken in parallel for different models on separate threads and then the results aggregated.

\item\textbf{R6.} Does our system outperform state-of-the-art regular expression scanning in terms of memory usage and testing/training time? What is the rate of scalability?

\textbf{F6.} As indicated in Table~\ref{table:performanceresults}, for small signature sets Hyperscan initially outperformed our model; however, when scaled to larger signature sets, our model clearly is much more capable of handling the extra load. The comparative line graphs in Figure~\ref{fig:hyperscancompare} illustrate the exponential versus sublinear growth patterns of each measure of performance, clearly demonstrating our systems' scalability over the state of the art. Our model takes just over 7 minutes to construct the LSH Forest with 1 million signatures, while the Hyperscan implementation takes over 2 hours. Even our testing time is much smaller than the Hyperscan test time at scale. For memory usage, our model also performs with reduced space complexity as the scale increases. At 1 million signatures, our model used just over 4GB of heap space, while Hyperscan crashed our setup due to exceeding 8GB at the same scale. If the problem set or data structure size is small, Hyperscan could be an optimal choice; however, our model is more generalizable to larger data sets and adaptable to a variety of problems. It would not be unreasonable to scale to several million signatures in a real surveillance or DPI system which \textsc{Alpine} and \textsc{Palm} would be better suited for than the current capability of regular expression matching.
\end{itemize}

\textsc{Alpine} and \textsc{Palm} show the potential for locality-sensitive measurements to consider multiple layers of features and are resilient to non-standard implementation. They normalize heterogeneous data into fixed-size, comparable space. These models as part of a DPI system have potential to scale both in performance and storage capacity. But, there are still problems in network classification that elude this method of feature extraction. For example, encrypted or compressed payloads make the text-based approach of \textsc{Palm} or regular expressions generated by RExACtor ineffective. Distinguishing encrypted traffic payloads from plaintext or compressed payloads is also a classification problem not appropriately dealt with through these methods. If \textsc{Alpine} features of the header are not enough alone to distinguish the traffic type or particular application the traffic represents, then the classification will be inaccurate. If the header is encrypted, this also renders \textsc{Alpine} ineffective. In further research, a method of understanding and profiling encrypted and compressed traffic or obfuscated payloads would greatly enhance deep learning-based DPI systems.
