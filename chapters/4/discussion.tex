\section{Discussion and Limitations}

The locality-sensitive hashing approach is an improvement upon the regular expression approach to DPI in several ways. First, the hash generation process reduces the need for expert knowledge about a particular protocol. Previously, engineers would have to employ unique signatures for different message types. It would not be uncommon, for example, to have regular expression signatures for every method for a given text-based protocol like FTP or IMAP. Furthermore, requests and response messages often require unique signatures. Regexes are difficult for humans to read and comprehend. The management of server-client side interactions is also a challenge to know which versions to apply. Instead, hash embeddings using our forward-thinking system create a unique value for each provided input and indexes those values into one bucket. This even more generally captures the broad diversity of possible payload and header contents while providing a new layer of abstraction and simplicity. Flows may be examined bi-directionally, and all message types and methods considered. We are also not limited to traffic flows and protocol types. As demonstrated by the experiments in this section, the system is capable of distinguishing more heterogenous or abstract class types like traffic type, mechanisms like encryption, and traditionally weak-signatured traffic like RTP. Furthermore, a single bad bit can cause a packet match on regular expression to fail. Network traffic and packets in the wild are extremely diverse and can be prone to error, and thus are an ideal test set for the multi-embedding hash technique which is both more informative and resilient. In terms of performance, the LSHForest as implemented in our model further reduces the storage and computational complexity when compared with a regular expression, automata-searching approach.

We posed the following research questions to assess the \textsc{Alpine} and \textsc{Palm} models and report our findings here:

\begin{itemize}
\item\textbf{R1.} Can packets be classified by application-layer potocol through the creation and index of locality-sensitive hashes?

Our model performed at a 99.6\% accuracy rate in its identification of many application layer protocols, indicating that locality sensitive hashes may be constructed which are highly representative of the data and low-cost for searching when indexed. This model may be expanded in real world implementations of DPI for traffic surveillance, cybersecurity and intrusion detection, quality of service managment and network management, content and copyright enforcement, and traffic profiling. Detecting the application layer protocol remains an important task for engineers. This hashing technique improves upon the available state of the art in both accuracy and breadth of scope in application.

\item\textbf{R2.} Does this model generalize to other kinds of network traffic classes besides protocol? Could we also distinguish more variable data such as email or VoIP traffic classes?

In addition to the application layer protocol, we also used traffic type labels on the dataset for experimentation. In this problem also the hash embedding technique proved highly accurate, which further demonstrates the generalizability of the models to new problems and real data.

\item\textbf{R3.} Does a different word embedding technique such as using only high-value tokens as characterized by TF-IDF score improve distance measure results? Will this reduce noise in the data and isolate important features?

We used Jaccard similarity as a baseline for comparison, but found that this distance metric was more effective than the TF-IDF score. This highlights a counterintuitive but significant point that challenges current network analysis trends. Machine learning techniques have become more technically advanced and often computationally intensive in order to improve model accuracy or other desired metrics. While TF-IDF decently diminishes tokens such as ``a'' and ``the'' from the English language, the fact that ``\textsc{HTTP\textbackslash1.1}'' appears in nearly every HTTP request in our dataset is actually relevant considering the context. Especially in network traffic analysis where scalability and line-rate capability is highly desirable, simple techniques can still be effective and should not be disregarded. Our experiments evidence that a more complex model is not always the appropriate direction. Even within subfields of machine learning like natural language processing, there is not a silver bullet solution which triumphs in every situtation. Rather, having multiple models to try or angles from which to consider the data seems to provide better, more well-rounded results.

\item\textbf{R4.} If we combined multiple hash embeddings, i.e. both \textsc{Alpine} and \textsc{Palm}, and concatenate their results, will there be an improvement in classification performance over a single hash embedding of the feature set?

Multi-embeddings are a proposed solution particularly for more heterogeneous datasets. This captures multiple angles or representations of the data and combines them for a more full-scope picture and analysis. In contrast, regular expressions provide only a one-dimensional viewpoint and are incapable of encoding highly variable data without impractical complexity and data over-selection. The modular design of our system enables using one or more embeddings, which allows for a balance between performance impact and accuracy. Interestingly, though the \textsc{Palm} model was overall poorer-performing than the \textsc{Alpine} model, votes from the \textsc{Palm} model helped improve the combined result regardless.

\item\textbf{R5.} Does the sample size, number of hashes, or number of classes significantly impact throughput or accuracy?

The performance impact of applied machine learning is a major concern for traffic analysis and cybersecurity. We provide transparent numbers of system performance to present the solution at scale. All experiments in this trial were run on a 1.6 GHz Dual-Core Intel Core i5 processor with 16GB DDR3 memory. In future work, leveraging an accelerated processor such as an FPGA would greatly enhance performance and throughput due to the repetitive nature of hash computations. The most impactful procedure based on system profiling is the MinHash step, which is likely optimizable through an FPGA~\cite{fpga}. The measure of acceptable throughput depends on the network environment; however, in terms of scalability it is valuable that our model is data independent in that it does not seem to increase exponentially or vary in performance wildly depending on the data. Our model also trains quickly, meaning that models could be swapped or trained online or in the field if needed. Future research could further optimize the process through batching and parallel processing. For example, it would be possible and beneficial to run training computations in parallel.Distance measures could also be taken in parallel for different models on separate threads and then the results aggregated.

\item\textbf{R6.} What is the rate of scalability of the system? Does it outperform state-of-the-art regular expression scanning in terms of testing/training time and memory usage?

We indicate in Table~\ref{table:performanceresults} that for small signature sets, Hyperscan initially outperformed our model; however, when scaled to larger signature sets, our model clearly is much more capable of handling the extra load. The comparative line graphs in Figure~\ref{fig:hyperscancompare} illustrate the exponential versus sublinear growth patterns of each measure of performance, clearly demonstrating our systems' scalability over the state of the art. In constructing models with 1 million signatures, our model takes just over 7 minutes while the Hyperscan implementation takes over 2 hours. At scale, even our testing time is much smaller than the baseline. Furthermore, at the 1 million signature scale our model uses just over 4GB of heap space, while Hyperscan crashed our setup due to exceeding 8GB of dynamic memory for 1 million signatures. This illustrates the point made earlier that no single model is best suited for every situation; rather, multiple models and strategies are needed for unique network environments. If the problem set or data structure size is small, Hyperscan could be an optimal choice; however, our model is more generalizable to larger data sets and adaptable to a variety of problems. It would not be unreasonable to scale to several million signatures in a real surveillance or DPI system which \textsc{Alpine} and \textsc{Palm} would be better suited for than the current capability of regular expression matching.
\end{itemize}

One key takeaway from the research done in \textsc{Alpine} and \textsc{Palm} is that multi-embeddings on packets in our given datasets and experiments were more effective at classifying the data than any single form of analysis. Naturally, this comes at the cost of generating more embeddings and thus increasing complexity and latency. Thus, when designing a multi-embedding model, there is a balance to be achieved between number of layers of representation and desired results. If storage space of models and embeddings or CPU utilization is of more concern than acute accuracy of classification, fewer or less complex representations may be required. The data may also be aptly divided for input into different models; for example, \textsc{Alpine} considers header features while \textsc{Palm} considers the payload.

\textsc{Alpine} and \textsc{Palm} show the potential for locality-sensitive measurements to consider multiple layers of features and are resilient to non-standard implementation. They normalize heterogeneous data into fixed-size, comparable space. These models as part of a DPI system have potential to scale both in performance and storage capacity. But, there are still problems in network classification that elude this method of feature extraction. For example, encrypted or compressed payloads make the text-based approach of \textsc{Palm} or regular expressions generated by \textsc{Rexactor} ineffective. Distinguishing encrypted traffic payloads from plaintext or compressed payloads is also a classification problem not appropriately dealt with through these methods. If \textsc{Alpine} features of the header are not enough alone to distinguish the traffic type or particular application the traffic represents, then the classification will be inaccurate. If the header is encrypted, this also renders \textsc{Alpine} ineffective. In further research, a method of understanding and profiling encrypted and compressed traffic or obfuscated payloads would greatly enhance deep learning-based DPI systems.
