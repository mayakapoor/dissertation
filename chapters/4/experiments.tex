\section{Experiments and Results}
\begin{table} [ht!]
\caption{Description of Combined Dataset}
\centering
\begin{tabular}{|c |c | c |}
\hline
\textbf{Application Type} & \textbf{Protocol} & \textbf{\# Samples} \\
\hline
\hline
\textit{File Transfer} & FTP & 10,015 \\
& FTP-DATA & 4,000 \\
& BitTorrent &  20,648 \\
\hline
\hline
\textit{Voice-over-IP} & MGCP & 1,568 \\
& SIP & 1,112 \\
& H.225 &  1,300 \\
& RTP &  15,552 \\
& RTCP &  1,626 \\
\hline
\hline
\textit{Mail} & SMTP & 5,981 \\
& POP & 1,675 \\
& IMAP &  3,318 \\
\hline
\hline
\textit{Authentication \& Access} & LDAP & 1,354 \\
& SMB &  3,554 \\
& Telnet & 1,888 \\
\hline
\hline
\textit{Tunneling} & GPRS & 9,981 \\
& PPTP & 1,288 \\
& SSH & 3,039 \\
\hline
\hline
\textit{Web \& Chat} & DNS & 10,563 \\
& HTTP & 21,298 \\
& XMPP &  1,553 \\
& TLS & 4000 \\
\hline
\hline
\textit{Networking} & DHCP & 1,444 \\
& NBNS & 1,216 \\
& GQUIC &  1,740 \\
& NTP & 1,940 \\
& SSDP &  8,504 \\
\hline
\end{tabular}
\label{table:dataset}
\end{table}

Using only a single dataset or network environment can lead to bias in results~\cite{Silva2022}. Thus, we created a combined dataset from many available public datasets in order to diversify the traffic and contents. Our sources include the CDX 2009 captures~\cite{cdx2009}, the Skynet Tor dataset~\cite{skynet}, the Canadian Institute for Cybersecurity's ISCX VPN/non-VPN and Tor/non-Tor datasets~\cite{vpn-dataset, tor-dataset}, and repositories from Wireshark, Cloudshark, and IEEE Dataport. This repository contains PCAPs from twenty-six different, common application layer protocols. For experimental reproducibility and general use we have made this new dataset available publicly~\footnote{https://github.com/mayakapoor/protocol-dataset}.


\subsection{Protocol Auto Detection}

\begin{table}
\caption{Protocol Autodetection Results}
\centering
\begin{tabular}{| c | p{0.6cm}  p{0.6cm}  p{0.6cm} || p{0.6cm}  p{0.6cm}  p{0.6cm} |}
\hline
& & \textbf{Jaccard} & & & \textbf{TF-IDF} & \\
\hline
\hline
\textbf{Accuracy} & & 0.996 & & & 0.841 & \\
\textbf{Cohen's $\kappa$} & & 0.997 & & & 0.841 & \\
\hline
\hline
 \textbf{Protocol} & \textbf{P} & \textbf{R} & \textbf{F1} & \textbf{P} & \textbf{R} & \textbf{F1} \\
 \hline
 BitTorrent & 1.00 & 0.99 & 0.99 & 0.89 & 0.85 & 0.86 \\
 DHCP & 1.00 & 1.00 & 1.00 & 0.91 & 1.00 & 0.95 \\
 DNS & 1.00 & 0.99 & 1.00 & 0.95 & 0.97 & 0.96 \\
 FTP & 0.99 & 1.00 & 0.99 & 0.83 & 0.90 & 0.86 \\
 FTP-DATA & 1.00 & 1.00 & 1.00 & 0.93 & 0.96 & 0.94 \\
 GPRS & 1.00 & 1.00 & 1.00 & 0.91 & 0.98 & 0.95 \\
 GQUIC & 1.00 & 0.99 & 0.99 & 0.87 & 0.75 & 0.81 \\
 HTTP & 1.00 & 0.99 & 0.99 & 0.82 & 0.48 & 0.61 \\
 H.225 & 1.00 & 1.00 & 1.00 & 0.83 & 0.97 & 0.90 \\
 IMAP & 1.00 & 0.99 & 0.99 & 0.86 & 0.79 & 0.82\\
 LDAP & 1.00 & 0.99 & 0.99 & 0.87 & 1.00 & 0.93 \\
 MGCP & 1.00 & 1.00 & 1.00 & 0.90 & 0.65 & 0.75 \\
 NBNS & 0.99 & 1.00 & 0.99 & 0.94 & 1.00 & 0.97 \\
 NTP & 1.00 & 1.00 & 1.00 & 0.91 & 1.00 & 0.95 \\
 POP & 0.98 & 1.00 & 0.99 & 0.91 & 0.47 & 0.62 \\
 PPTP & 0.95 & 1.00 & 0.98 & 0.84 & 0.90 & 0.87 \\
 RTCP & 1.00 & 1.00 & 1.00 & 0.96 & 1.00 & 0.98 \\
 RTP & 1.00 & 1.00 & 1.00 & 0.89 & 0.88 & 0.89\\
 SIP & 1.00 & 1.00 & 1.00 & 0.54 & 0.98 & 0.69 \\
 SMB & 1.00 & 0.97 & 0.98 & 0.95 & 0.99 & 0.97 \\
 SMTP & 1.00 & 0.99 & 0.99 & 0.92 & 0.93 & 0.93 \\
 SSDP & 1.00 & 1.00 & 1.00 & 0.72 & 0.28 & 0.40 \\
 SSH & 1.00 & 0.98 & 0.99 & 0.90 & 0.86 & 0.88 \\
 Telnet & 0.99 & 1.00 & 0.99 & 0.86 & 0.99 & 0.92 \\
 TLS & 1.00 & 1.00 & 1.00 & 1.00 & 1.00 & 1.00 \\
 XMPP & 1.00 & 1.00 & 1.00 & 0.79 & 0.90 & 0.84 \\
 \hline
\end{tabular}
\label{table:results1}
\end{table}

It is crucial for lawful interception and DPI middleware boxes to be able to autodetect application-layer protocols in order to properly assess, parse, and process seen traffic. The environment on any given signal can be wildly diverse; thus, classification systems must be capable of scaling to large, multiclass problems. Previous systems~\cite{hslf, fpga} have not attempted protocol detection to the scale of a 26-class problem, making this model a true and fresh pioneer in the DPI field. We first split our data into training and test sets and extracted features from each of the protocols. Next, we generated hash values for the training samples and indexed them into the forest. For testing, we generate a hash of the input value and perform the similarity search on the trained model. The results of the testing samples are provided in Table~\ref{table:results1}. The evidence shows that the combined model has high accuracy and precision/recall for protocol identification. Cohen's kappa also shows a strong agreement among the ensemble voters. The method also performs equally well across application types, indicating generalizability. Figure~\ref{fig:combined} shows the confusion matrix results of the classifier's ensemble voting system.

\begin{figure} [ht!]
  \centering
  \includegraphics[width=\columnwidth]{chapters/4/img/combined.png}
  \caption{Twenty-six Class Identification Problem using the \textsc{Alpine}-\textsc{Palm} Combined Model}
  \label{fig:combined}
\end{figure}

\subsection{Traffic Type}

Standardized documentation necessitates that there are some detectable similarities between protocols. It is reasonable to think that detection of more abstract classes like traffic type (i.e. email, file transfer, web data) would be more heterogeneous and thus more difficult to detect by embedding and similarity search. We experimented with classification by traffic type by amalgamating the dataset into classes as described in Table~\ref{table:dataset} in the datasets section of this work. Results in Figure~\ref{fig:trafficresults} show a confusion matrix of the classification results which are also highly accurate.

\begin{figure} [hbt!]
  \centering
  \includegraphics[width=0.8\columnwidth]{chapters/4/img/traffic_class_results.png}
  \caption{Confusion Matrix of Traffic Type Classification Results}
  \label{fig:trafficresults}
\end{figure}

\subsection{TF-IDF Score Evaluation}

There are many complex ways to measure similarity between two items; Jaccard similarity is considered relatively simple. In order to reason whether or not the similarity metric was effective, we used the TF-IDF score of payload tokens in order to filter out low-value tokens and generate hashes only from high-value ones. This metric was chosen to address concern that large data payloads, for example HTML documents or email messages, might introduce noise into hashes which would reduce classification accuracy. Instead, isolating payload tokens to only the high value ones could reduce spurious extra data. The TF-IDF measurement of tokens was implemented in the training stage by modifying configurations of the \textsc{Palm} model to filter the added tokens. Only those tokens with TF-IDF score above the minimum threshold were kept and included in the actual LSH signature. The results in Table~\ref{table:results1} show that this actually had a negative impact on the classification accuracy, indicating the basic Jaccard similarity was a better distance metric for this use case.

\subsection{Multi-Embeddings}

\begin{table}
\caption{Multi-Hash Embedding Classification Results}
\centering
\begin{tabular}{| p{2cm} | p{0.6cm} p{0.6cm} p{0.6cm} || p{0.6cm} p{0.6cm} p{0.6cm} || p{0.6cm} p{0.6cm} p{0.6cm}|}
\hline
& \textbf{\textsc{Alpine}} & & & \textbf{\textsc{Palm}} & & & \textbf{Multi-Model} & & \\
\hline
\hline
\textbf{Accuracy} & 0.988 & & & 0.717 & & & 0.996 & & \\
\textbf{Cohen's~$\kappa$} & 0.988 & & & 0.705 & & & 0.997 & & \\
\hline
\hline
 \textbf{Protocol} & \textbf{F1} & \textbf{P} & \textbf{R} & \textbf{F1} & \textbf{P} & \textbf{R} & \textbf{F1} & \textbf{P} & \textbf{R} \\
 \hline
 BitTorrent & 0.98 & 0.97 & 1.00 & 0.74 & 0.75 & 0.73 & 0.99 & 1.00 & 0.99 \\
 DHCP & 1.00 & 1.00 & 1.00 & 0.91 & 0.83 & 1.00 & 1.00 & 1.00 & 1.00 \\
 DNS & 1.00 & 1.00 & 1.00 & 0.24 & 0.46 & 0.17 & 1.00 & 1.00 & 0.99 \\
 FTP & 0.96 & 0.97 &  0.94 & 0.90 & 0.83 & 0.98 & 0.99 & 0.99 & 1.00 \\
 FTPDATA & 1.00 & 1.00 & 1.00 & 0.20 & 0.37 & 0.13 & 1.00 & 1.00 & 1.00 \\
 GPRS & 1.00 & 1.00 & 1.00 & 0.91 & 0.98 & 0.85 & 1.00 & 1.00 & 1.00 \\
 GQUIC & 0.99 & 0.99 & 1.00 & 0.27 & 0.48 & 0.19 & 0.99 & 1.00 & 0.98 \\
 HTTP & 0.99 & 1.00 & 1.00 & 0.61 & 0.65 & 0.55 & 1.00 & 1.00 & 0.99 \\
 H.225 & 1.00 & 1.00 & 1.00 & 0.63 & 0.68 & 0.95 & 1.00 & 1.00 & 0.99 \\
 IMAP & 0.99 & 0.94 & 0.98 & 0.85 & 0.77 & 0.59 & 0.99 & 1.00 & 0.99 \\
 LDAP & 0.96 & 1.00 & 0.98 & 0.66 & 0.75 & 0.59 & 0.99 & 1.00 & 0.99 \\
 MGCP & 1.00 & 1.00 & 1.00 & 0.90 & 0.83 & 1.00 & 1.00 & 1.00 & 1.00 \\
 NBNS & 0.99 & 0.98 & 1.00 & 0.87 & 0.82 & 0.92 & 0.99 & 0.99 & 1.00 \\
 NTP & 1.00 & 1.00 & 1.00 & 0.90 & 0.82 & 0.99 & 1.00 & 1.00 & 1.00 \\
 POP & 0.99 & 0.98 & 0.99 & 0.26 & 0.41 & 0.20 & 0.99 & 0.98 & 1.00 \\
 PPTP & 0.99 & 0.98 & 0.99 & 0.49 & 0.33 & 1.00 & 0.98 & 0.95 & 1.00 \\
 RTCP & 1.00 & 1.00 & 1.00 & 0.92 & 0.85 & 1.00 & 1.00 & 1.00 & 1.00 \\
 RTP & 0.99 & 0.98 & 0.99 & 0.40 & 0.65 & 0.29 & 1.00 & 1.00 & 1.00 \\
 SIP & 1.00 & 1.00 & 1.00 & 0.93 & 0.87 & 1.00 & 1.00 & 1.00 & 1.00 \\
 SMB & 0.99 & 1.00 & 0.98 & 0.81 & 0.80 & 0.82 & 0.98 & 1.00 & 0.97 \\
 SMTP & 0.98 & 0.99 & 0.98 & 0.81 & 0.80 & 0.82 & 0.99 & 1.00 & 0.99 \\
 SSDP & 0.99 & 1.00 & 0.98 & 0.92 & 0.86 & 1.00 & 1.00 & 1.00 & 1.00 \\
 SSH & 0.95 & 0.97 & 0.92 & 0.47 & 0.67 & 0.37 & 0.99 & 1.00 & 0.98 \\
 Telnet & 0.99 & 0.98 & 1.00 & 0.80 & 0.75 & 0.85 & 0.99 & 0.99 & 1.00 \\
 TLS & 0.99 & 0.99 & 1.00 & 0.61 & 0.51 & 0.75 & 1.00 & 1.00 & 1.00 \\
 XMPP & 1.00 & 1.00 & 0.99 & 0.92 & 1.00 & 0.85 & 1.00 & 1.00 & 1.00 \\
 \hline
\end{tabular}
\label{table:embeddingresults}
\end{table}

It has been shown that concatenating multiple classifiers and evaluating the agreeance among voters yields a reduction in error and even correlation when there is a misclassification~\cite{tumerensemble}. We hypothesized that enabling both \textsc{Alpine} and \textsc{Palm} to run together would yield the most accurate and agreed-upon classification results. We tested this theorem by running the 26-class protocol identification test against configurations of the model with just \textsc{Alpine} embedding, only \textsc{Palm} embedding, and a final combined run. Results as shown in Table~\ref{table:embeddingresults} demonstrate that the combined votes of both models were the most accurate in the final tests. Furthermore, the agreeance among voters was also highest using multiple hash embeddings.

\subsection{Model Performance and Throughput}

Applied machine learning must keep up with signal processing at very high rates; furthermore, any passive system must not interfere with legitimate traffic and service and can thus not introduce additional latency or the potential for dropped packets. The software must also be capable of performing the required processing for data analysis on as much of the traffic as possible (ideally, all of it). Diverting or copying traffic offline for analysis as well as parallelization strategies and traffic thinning and load balancing are legitimate system design choices which can mitigate this. But there is still room for improvement and optimization at the classification solution level to achieve real-time rates. Thus, we perform experiments to see how the system scales based on model sizes and number of classes. We measured the training time, system throughput in millibits per second (Mbps) as well as memory usage during the testing phase in megibytes (MiB) as we were running all performance tests on the combined dataset consisting of 140,157 total packets. Random under-sampling was used to even out the distribution of data and avoid bias, which caused our total number of packets after sampling to be a fraction from the original set. For testing the number of classes, we created experiments with binary RTP/non-RTP classification, the traffic-type multiclass experiment with 8 classes/types, and the largest 26-class experiment where we identify all possible data layer protocols. In Table~\ref{table:performanceresults}, we detail the results of the experiments for a varied number of classes and sample sizes.

\begin{table*} [ht!]
\caption{Performance Results for Varied Number of Classes}
\centering
\begin{tabular}{| p{0.11\linewidth} | p{0.08\linewidth} | p{0.09\linewidth} | p{0.09\linewidth} | p{0.09\linewidth} | p{0.09\linewidth} | p{0.13\linewidth} | p{0.1\linewidth} |}
\hline
Classes & \# Hashes & \# Samples & Training Time & Test Time & Memory Usage & Throughput & Accuracy \\
\hline
\hline
\textbf{RTP/ non-RTP} & 20,613 & 13,743 & 4.273s & 1.21ms & 470.8 MiB & 2.596 Mb\/s & 1.000 \\
\textit{Binary classification} & 2,400 & 1,600 & 0.738s & 0.866ms & 283.7 MiB & 3.139 Mb\/s & 0.997 \\
& 240 & 160 & 0.075s & 1.093ms & 353.4 MiB & 2.317 Mb\/s & 1.00 \\
\hline
\textbf{Traffic Type} & 28,543 & 19,029 & 7.136s & 1.015ms & 584.6 MiB & 3.728 Mb\/s & 0.997 \\
\textit{8-class problem} & 12,600 & 8,400 & 3.31s & 1.006ms & 305.3 MiB & 2.842 Mb\/s & 0.997 \\
& 240 & 160 & 0.683s & 2.397ms & 290.6 MiB & 2.54 Mb\/s & 0.998 \\
\hline
\textbf{Protocol} & 17,347 & 11,565 & 5.312s & 1.138ms & 352.4 MiB & 3.041 Mb\/s & 0.997 \\
\textit{26-class problem} & 15,600 & 10,400 & 3.13s & 0.727ms & 305.3 MiB & 4.231 Mb\/s & 0.998 \\
& 1,560 & 1,040 & 0.492s & 0.901ms & 301.8 MiB & 3.212 Mb\/s & 0.994 \\
\hline
\end{tabular}
\label{table:performanceresults}
\end{table*}

In order to test system scalability in deployment and verify the theoretical complexity discussed in the background section of this work, we performed a series of experiments increasing the number of signatures for a model used to classify HTTP traffic. For a comparitive baseline, we used the regular expression scanning tool built in \textsc{Rexactor}~\cite{rexactor} which uses the Hyperscan 5.4.0 regular expression matching library by Intel~\cite{hyperscan}. All experiments were performed on a single i9 Dell computer with Ubuntu 18.04 installed. In the trials, the number of signatures represented the number of LSH hashes our system or the number of regular expressions added to the sniffer application. We used \textsc{Rexactor}~\cite{rexactor} to generate HTTP signatures from the HTTP traffic in our dataset. Both models performed the classification with 100\% accuracy on all trials.

\begin{table}[ht!]
\centering
\begin{tabular}{| c | c | c | c | c |}
\hline
System & \# Signatures & Training & Testing & Memory \\
\hline
\hline
\textbf{\textsc{Alpine}/\textsc{Palm}} & 1,000 &  1.21s &  1.23s & 392.4 MiB \\
& 10,000 & 2.75s & 3.56s & 419.9 MiB \\
& 100,000 & 52.75s & 4.2s & 1047.4 MiB \\
& 1,000,000 & 423.3s & 4.52s & 4717.9 MiB \\
\hline
\textbf{Hyperscan} & 1,000 & 4.6s & 156ms & 70.7 MiB \\
& 10,000 & 81.8s & 1.3s & 529.3 MiB \\
& 100,000 & 39.9m & 17.28s & 4596.7 MiB \\
& 1,000,000 & 153m & 63.1s & - \\
\hline
\end{tabular}
\caption{Performance Results for \textsc{Alpine} and \textsc{Palm} as a Multi-embedding Versus Hyperscan}
\label{table:performanceresults}
\end{table}

\begin{figure} [ht!]
  \centering
  \includegraphics[width=0.6\columnwidth]{chapters/4/img/trainingtime.png}
  \includegraphics[width=0.6\columnwidth]{chapters/4/img/testingtime.png}
  \includegraphics[width=0.6\columnwidth]{chapters/4/img/memoryusage.png}
  \caption{Time and Space Complexity Results for \textsc{Alpine}/\textsc{Palm} versus Hyperscan}
  \label{fig:hyperscancompare}
\end{figure}
