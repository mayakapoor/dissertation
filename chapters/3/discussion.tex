RExACtor provides a classification solution for per-packet analysis and does not require data to be read as or reconstructed into flows in order to achieve classification. The modular design of RExACtor also allows for components to be used as tool sets so that engineers may adapt the knowledge discovery elements to their own scanning systems or use the full system as a whole framework. For example, the module TRex which derives tokens may be run individually in order to create keywords for Snort filters. Additionally, regular expression signatures can be created and published outside of the scanner in order to be plugged into any other DPI application. RExACtor also supports tokenization and alignment for raw data, allowing for analysis and signature creation for unknown protocols. This high adaptability means RExACtor can be applied in many network environments for knowledge discovery and DPI usage.
Currently, RExACtor supports email (SMTP, POP3, IMAP), file transfer (FTP), chat (XMP, IRC), VoIP (SIP, RTSP), and web (HTTP) protocol analysis. Because RExACtor utilizes unsupervised learning and derives features from training data, it could be applied to learning more application specific signatures. One improvement for our solution and others discussed which use unsupervised learning would be an attempt to learn an optimal threshold for feature support rather than rely on manual configuration. In the comparison study, we found that packets not identified by the signatures were HTTP continuation packets of just data payload. Sessionization of protocol flows would allow a detection system to pick up on the first HTTP packet of an exchange with identifying method information, then record the 5-tuple flow information for further scanning. But this technique is not usable in per-packet analysis, and is a limitation of the problem space rather than our solution.

Regular expressions as a text-based approach also have limitations to their applicability in the real, complex network environment. The current industry standard for application layer DPI is to use regular expressions to match expected values in payloads. Protocols or applications may have multiple signatures to match request, response, or message types. Some protocols also do not have strong signatures. Furthermore, different versions of protocols or updated standards and RFCs often require separate signature sets and the continual update of older patterns~\cite{rexactor}. Some protocols are also so non-standard that it is difficult to track them on a regular expression signature alone; for example, the payload of an RTP packet is raw data and assigned ports are dynamic~\cite{rfc3550}. Current state-of-the-art scanners suffer from variable and data-dependent performance impact~\cite{deepmatch}. With the increasing size and complexity of rulesets and the growing scale of the network environment, regular expression scanning using the current systems is more and more impractical. Regex methods are also limited by what classes they can be used for in network traffic. For example, there is no real regular expression signature which can identify Tor-routed traffic from non-Tor-routed traffic; the data is too diverse. Other desirable classes for traffic, such as traffic type, application, and user or device type present similar issues to regular expression scanners. Each possiblity must have its own, human-engineered regular expression to match precisely which is both slow and prone to error. In addition to scalability and machine performance, text-based regular expression matching no longer meets the needs of the environment as over 90\% of browsing data today is encrypted~\cite{google}.

Regular expression matching which relies on deterministic and non-deterministic finite automata (DFA and NFA) is prone to the state explosion problem, where computational complexity increases exponentially as regular expressions increase in number and  become more complex. DFAs are fast to search, but are most prone to state explosion as every possibility is explicitly constructed. NFAs provide linear storage space to the size of the regular expression ruleset, but it is relatively easy to arrive at worst-case search performance with rule complexity~\cite{alpinepalm}. ~\cite{Fu}. Table~\ref{table:facomplex} shows the processing complexity and storage costs of these structures where $m$ is a number of regular expressions of length $n$.

\begin{table} [ht!]
\caption{Worst-Case Space and Time Complexities for NFA and DFA~\cite{Yu}}
\centering
\begin{tabular}{|c | c | c|}
\hline
\textbf{Data Structure} & \textbf{Complexity} & \textbf{Storage Cost} \\
\hline
NFA & $O(n^{2}m)$ & $O(nm)$ \\
\hline
DFA & $O(1)$ & $O(\Sigma^{nm})$ \\
\hline
\end{tabular}
\label{table:facomplex}
\end{table}

Recently, hybrid NFA/DFA engines have been proposed to improve search performance; however, improvements on computational complexity can still be prone to memory issues in the case of large regular expression rulesets~\cite{hyperscan}. Parallel computing may alleviate some of the computational stress, but is acknowledged as a brute force approach~\cite{Fu}. Hyperscan is able to handle compiling and searching with incredible efficiency; however, this becomes exponentially worse to the point of impracticality when put at a test to scale, which we demonstrate in the next chapter of this work.
