Perhaps the most critical cybersecurity interface in computing today is the connection an individual has to the worldwide Internet. More than an entire petabyte of data is sent over the Internet every second. The majority of traffic is broken down into transportable, encapsulated units of data called packets. These packets contain multiple layers of network transport and infrastructure information wrappering application and user data known as the payload. These layers are defined by Internet protocols, which are standardized by the Internet community in technical documentation. While the original intent of this data is benign, malicious software downloads, viruses, and spyware are just a few examples of what may be hidden inside packet payloads. Even benign data is not safe, as attackers may be trying to exfiltrate sensitive information such as enterprise or government secrets, national intelligence, or personally identifiable information. Futhermore, it is not just attackers surveying this data, as lawful interception must be performed for Internet regulation, network administration, counter cyberterrorism, and criminal investigation.

As of January 2023, there are over 5.16 billion users on the Internet. At this scale, there is an urgent need for efficient and accurate packet inspection and classification. Network traffic classification is the process of identifying packets by characteristics such as application, application-layer protocol, traffic type such as email or chat, by user or device, or some other class as defined by the interested party. The goal is to identify traffic of interest or of a particular type, perhaps for further data analysis if acting as a man-in-the-middle or for administrative purposes over the network. Network intrusion detection systems (NIDS) and prevention systems (NIPS) can be thought of as lawful interceptors who also rely on classification and identification techniques to locate malware signatures in payloads, perform dynamic access control, discover malicious flows and processes, and detect anomalies based on metadata features. Classification is also used in Internet traffic monitoring systems, or sniffers. Law enforcement agencies and intelligence organizations require the ability to collect, sessionize, and analyze streams of traffic. For law enforcement agencies, network packets contain information which can be routed back to crimes and cyber threats, and big data analytics can help reconstruct these traces into useful evidence.

In order to use deep learning and data mining techniques to perform network traffic classification, features must be extracted from packets to use as input and define classes. Techniques such as dynamic port allocation, encryption and compression of payloads, spoofing, and tunneling have significant impact on the ability of systems to extract meaningful features from packets. Furthermore, these features can be of high dimensionality or heterogenous and must be normalized and reduced to a processable, comparable format for machine learning algorithms. In this work, we explore techniques in data extraction and transformation of packets to uncover \textit{hidden representations} for learning. We then develop and optimize models for processing and classifying data based on these derived features. The primary contribution of this dissertation is the systemization of these techniques and models into a network traffic classification toolkit ~\textsc{(\textbf{Forager})} which will be released and made available for public use.

\section{Problem Statement}

Packet classification is not one-dimensional; an individual packet may be accurately classified in multiple ways. For example, a packet may be sent by user Bob containing email data characterized by the POP3 application-layer protocol. Depending on the intent of the classifier, the same packet may be classified as email traffic, encrypted or compressed, benign traffic, traffic belonging to user Bob, or traffic containing POP3 data. One limitation of existing research is that solutions tend to address only one classification problem or a subset of issues without recognizing or expanding to others. In this section, we introduce several of the most pressing problems to network traffic classification today: traffic identification and profiling, network architecture complexity, data obfuscation, and system scalability.

\subsection{Identification and Profiling}

Protocol and data exchanges in the modern Internet can be complex over the lifetime of the session exchange, particularly for large amounts of transferable data. Specifically, routines like Voice-over-IP (VoIP) calls, file transfers and downloads, streaming services, and peer-to-peer sharing can transmit hundreds of thousands of packets of variable data over several minutes to hours. These exchanges are also not guaranteed to be one-to-one; for example, content servers may be queried by hundreds of thousands of users at once for mass streaming services. Additionally, there is not a one-size-fits-all approach to protocol identification. Some protocols have strong signatures or commonalites; for example, HTTP requests tend to have methods like \texttt{GET} or \texttt{POST} present. SIP is similar, as it has methods like \texttt{INVITE} and \texttt{BYE}. Thus, it is easy and quick to search for these by keyword or regular expression. In contrast, the RTP data conveyed by VoIP is weakly signatured. Finally, for sessionization or reconstruction of whole data flows (for example, reconstructing a video call), multiple protocols such as SIP and RTP must be correctly identified and associated together in real time.

Classification granularity could also be reduced to traffic type such as email or chat. This can be a more difficult problem as the features become more diverse across the individual classes. Still, several systems have successfully performed this traffic profiling even in encrypted and tunneled environments~\cite{iscx-tor-paper, iscx-vpn-paper, deeppacket, didarknet}. Many of these systems require entire traffic flows in order to perform this task, but we focus on single packet classification and other systems which follow the same paradigm.

\subsection{Network Architecture}

VPNs (virtual private networks) are private network infrastructures configured on top of public Internet services. VPNs can be used for increased security, privacy, and anonimity. Furthermore, administrators may control how data is transmitted and what data can be accessed by whom according to the network configurations. Depending on the site configuration, layers of tunneling, or dynamic mapping of networks, VPNs can add layers of Internet protocol (IP) encapsulation which obfuscates addresses. \textit{IP-over-IP encapsulation} means that the initial protocol stack of a stream can contain several layers of IP which may change over time. In many intrusion detection systems, surveillance equipment, and other middlebox technologies, 3-tuple or 5-tuple information made up of source and destination addresses and port numbers are used to track streams. If layers are added or changed, this can interrupt stream tracking and cause unintended breaks or loss of streams. Attackers or covert operators may wish to avoid tracking by switching tunnels, as well. Without the ability to track content streams regardless of network layer changes, the data law enforcement and the intelligence community care about will become lost and untraceable.

Tor ("The Onion Routing")~\cite{tor} is an internet framework intended to combat fingerprinting to prevent third-party tracking and surveillance through multiple layers of encryption. The Tor browser can be freely downloaded and used by anyone to achieve virtual network anonymity and access to sites which may be otherwise blocked for certain users or regions. It accomplishes multiple layers of encryption through a series of network hops where each server provides unique encryption keys which must be wrapped on and then peeled off at each stop. Over two million people employ Tor services as of 2022; The vast majority of Tor usage is benign but studies show a significant portion of users do perform illegal activities on the network and disproportionately so in countries which have less Internet regulation and government surveillance~\cite{Jardine2020ThePH}. Thus, it can be useful for surveillance and forensic operations to identify Tor traffic.

\subsection{Data Obfuscation and Spoofing}

In order to bypass firewalls or avoid detection, threat actors may choose to send traffic over non-standard ports or attempt to spoof traffic as a particular content type by sending it over a known port. There are many reasons to send non SSL/TLS-encrypted traffic over port 443. One primary motivation is that most firewalls default-allow traffic on ports 443 and 80. Thus, traffic may pass through alongside other data without the need for additional configuration or permissions. Devices which may wish to configure with a local network with minimal user or administrator overhead, such as Internet of Things (IoT) devices, have been discovered to send cleartext traffic alongside the encrypted traffic on these ports. Where security is not regulated or enforced, encryption standards also may just be ignored for simplicity's sake. Because the traffic is typically encrypted, payload-based deep packet inspection is often left unemployed on port 443 in favor of using compute resources on other traffic channels. Thus, port 443 has the potential to be used as a covert data channel if left unscanned. This has been realized in the wild; research has uncovered applications of foreign origin on public university networks running traffic through port 443 in order to avoid firewall detection. Threat actors may route data intended for other commonly scanned ports, such as email traffic and file transfer data, through 443 to avoid content-based inspection. Tunneling protocols like SSH may also be run using HTTPS to create encrypted covert data channels.

Traffic characteristics such as encryption, compression, and encoding can also be useful information for content inspection. Analysts may find plaintext traffic on port 443, for example, where they usually expect encrypted traffic. Being able to filter out and then further investigate this content could reveal important information related to surveillance or forensic operations. This also applies to filtering compressed traffic from encrypted traffic, as compression is easily reversible. Distinguishing these two types can be difficult on payload analysis alone as both methods produce highly entropic data, whereas plaintext data is much less entropic. Finally, determining encryption type and compression type are both required steps to completing any decompression or decryption task. Thus, network traffic classification requires systems capable of performing encryption and compression detection on large traffic streams.

As previously mentioned, IP-over-IP encapsulation and other dynamic network configurations can cause problems for tuple-based stream tracking which is often used in routing and surveillance equipment. While sometimes inadvertent, this can be considered another layer of obfuscation. When this network layer information changes, middlebox technologies must be capable of resilient adaptation to find and continue profiling particular user streams.

\subsection{Scalability}

Machine learning and deep learning based systems are plagued by computational complexity and stream buffering requirements. In much of the existing research, entire traffic flows or certain portions of flows are required before classification can begin. In a system at scale processing terabits of data per second, it is not practical to buffer this many streams in dynamic memory. Furthermore, classification speeds and models must be capable of accelerated computation through hardware or simple enough to be run in parallel quickly to keep up with line rate. For real-time classification, it may not be possible to wait for multiple packets in a single stream to identify a particular flow. Even in offline forensic analysis, entire streams may not be available or recoverable. Rather, the system must make a best effort guess without knowing the state of the traffic as to what application layer protocol is being carried for immediate parsing and processing. If possible, classifying the application layer traffic per packet would provide the lowest latency and highest throughput possible in the system.

\section{Proposed Solution}

The process of network traffic classification can be broken down into steps: deep packet inspection, representation learning, and machine learning-based classification. The task of identifying or classifying network traffic can begin with shallow or deep packet inspection. In these steps encapsulation layer and payload information of the packet is analyzed for matching content or features indicative of a particular class. This data may be further expanded through forms of representation learning or data transformation, where hidden features may be uncovered and used as input to the next step, machine learning. These algorithms such as neural networks or state vector machines learn these representations as classes and use the embedded data for the future testing phase in order to appropriately group incoming data. At runtime, a result for classification is returned on the input data. In this work, we provide novel contribution in applying several new forms of representation learning through data mining and transformation steps, as  well as appropriate deep learning classifiers following the creation of the hidden features.

This work proposes \textsc{Forager} as a toolbox for network traffic classification using data mining and deep learning methods. It is a culmination of lessons learned through the development of these modules and appropriately addresses the problems discussed in the scope of this dissertation. For scalability, \textsc{Forager} uses single-packet inspection techniques which may also be run in parallel for ultimate optimization. We demonstrate high success and negligible performance impact in protocol autodetection, traffic type and application profiling, and user fingerprinting. The system is also able to adapt to port obfuscation, spoofing, and tunneling architecture and identify VPN and Tor routing. Other types of data information such as compression, compression type, and encryption are able to be profiled through the \textsc{Forager} models.

The scope of this work focuses on traditional network data. Specialized datasets such as Internet of Things networks, mobile networks, and vehicular networks are not considered here but would be future opportunities for expansion. We also intentionally do not consider traffic flows or flow-based features. The data inputs and classifications performed in this work are done on a per-packet basis for minimized latency and buffering. While hardware optimizations and accelerations are discussed, we do not present experiments with hardware configurations due to setup constraints and problem scope. The focus of this work is the software implementations of data engineering, transformation, and analysis. Scalability is assessed through runtime metrics such as testing speed, throughput of data through the system (transformation plus classification), and dynamic memory usage. The models are also guaged with performance metrics from machine learning such as precision, recall, and F1-score.

\section{Contributions}

As part of this dissertation, we combined packets and traffic flows from multiple previously published PCAP sources and extracted their features into a diverse collection of tabularized data containing twenty-six different application layer protocols. We provide this dataset as a public resource for the community~\footnote{https://github.com/mayakapoor/protocol-dataset}. Each of the models which comprise \textsc{Forager} further the knowledge of their respective communities in the following ways.

\textbf{\textsc{Rexactor}} is our automatic regular expression generation solution detailed in Chapter 3. It furthers the state-of-the-art in regular expression generation by:

\begin{itemize}
\item adding a component tool for automatically extracting frequent and certain tokens from packet payloads,
\item applying genetic sequencing for substring alignment combined with frequent tokens,
\item encoding substrings and tokens in an original algorithm for more enriched, expressive regular expressions,
\item adding an additional tool for regular expression scanning using state-of-the-art automata software. \end{itemize}

\textbf{\textsc{Palm}} (\textbf{PA}yload \textbf{A}nalysis using \textbf{L}ocality-Sensitive \textbf{M}easurements) and \textbf{\textsc{Alpine}} (\textbf{A} \textbf{L}ocality-Sensitive \textbf{P}acket \textbf{IN}spection \textbf{E}ngine) can be found in Chapter 4 of this dissertation and provide the following contributions:

\begin{itemize}
\item A process for generating multiple locality-sensitive hash embeddings from packets which is highly accurate for identification of many classes, including protocol type, traffic type, application, and more,
\item A generalizable framework which applies to many network traffic classification problems, and whose model can be quickly trained and adapted to suit new problems,
\item An alternative to regular expression scanning for DPI which scales sublinearly and requires a linear amount of storage space,
\item A diverse and unique application of the traffic classification problem with experiments classifying multiple classes of protocols across many traffic types,
\item and public datasets and source code for experimental reproducibility.
\end{itemize}

Our \textbf{MA}trix \textbf{P}ay\textbf{L}oad \textbf{E}ncoder, \textbf{\textsc{Maple}}, and \textbf{D}ensity-based \textbf{A}nalysis \textbf{T}ensor \textbf{E}ncoder, \textbf{\textsc{Date}}, in Chapters 5 and 6 of this work additionally contribute:

\begin{itemize}
\item An algorithm for encoding packet payloads to image-based embeddings for latent representation,
\item an empirical evaluation and comparison of CNN models on RTP data,
\item an algorithm for generating three-dimensional point clouds from packet data, creating spatial latent representations as a novel method of packet analysis,
\item a novel application of density-based cluster analysis on packet payloads.
\end{itemize}

Each model is assessed with measures of precision, recall, and F1-score in different classification problems. We also provide comparison between models in order to better understand which transformations work better for different network environments and different traffic types or problems. In several of the experiments we also compare against state of the art work such as the Hyperscan regular expression scanning library or other machine learning profiling techniques and systems to show our system contribution. Our system provides an innovation also in its multi-modality, which we demonstrate in a case study in chapter 7 where we analyze traffic in multiple ways on port 443. We also assess memory usage and throughput across scenarios to propose the system at scale so that users may be able to design their classification strategy to their own requirements and available resources. As a final contribution we will provide \textsc{Forager} 1.0 as a downloadable software package.

\section{Summarization}

In the following chapters, we first explain the background work behind deep packet inspection and the path toward deep-learning based traffic classification. Chapter 3 introduces our work into automatic regular expression signature generation (\textsc{Rexactor}), which learns commonalities between packet payloads, transforms them into regular expressions, and performs text-based automata searching of the data. In chapter 4, systems based on locality-sensitive hashing (\textsc{Alpine} and \textsc{Palm}) introduce data compression and fixed storage space along with a decision-tree based forest classifier. We also introduce multimodality, a recurring enhancement to classification in our system. chapters 5 and 6 introduce \textsc{Maple} and \textsc{Date} respectively, which perform two-dimensional and three-dimensional transformations of data. In the \textsc{Maple} architecture, we further apply CNN classification. In \textsc{Date}, density-based clustering analysis is performed and statistics used as input to a neural network classifier. Finally, the ensemble system introduced in chapter 7 is the culmination of the previous models and the basis for \textsc{Forager} as a useable application for real cyber engineers and surveillance specialists. In chapter 8 of this work, we present a timeline for \textsc{Forager} development and completion. Chapter 9 concludes with further research opportunities and a summarization of issues discussed.
