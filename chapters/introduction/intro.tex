More than an entire petabyte of data is sent over the internet every second~\cite{verma2022}. At this scale, the need for efficient and accurate systems for packet inspection and classification is becoming more and more urgent. Network traffic classification is the process of identifying packets by characteristics such as application, application layer protocol, traffic type such as email or chat, by user or device, or some other class as defined by the interested party. The goal is to identify traffic of interest or of a particular type, perhaps for further data analysis if acting as a man-in-the-middle or for administrative purposes over the network. This technique is critical in both network security and systems engineering. Cisco as an administrator uses classification for enabling quality-of-service features like fast-forwarding and buffering prioritization \cite{Cisco}. Network intrusion detection systems (NIDS) and prevention systems (NIPS) can be thought of as lawful interceptors who also rely on classification and identification techniques to locate malware signatures in payloads, perform dynamic access control \cite{DIAS2019143}, discover malicious flows and processes, and detect anomalies based on metadata features \cite{Boger}. Classification is also used in internet traffic monitoring systems, or sniffers. Law enforcement agencies and intelligence organizations require the ability to collect, sessionize, and analyze streams of traffic. For law enforcement agencies, network packets contain information which can be routed back to crimes and cyber threats, and big data analytics can help reconstruct these traces into useful evidence~\cite{actionable-intelligence}.

Inspection of individual packets has been explored with data mining and deep learning techniques, but there are many class problems which have not been covered but are still necessary to real life analytics. Additionally, techniques explored previously do not meet the mission need of per packet classification at scale, requiring multiple packets, flow-based features, or heavy computational complexity. To better achieve accurate and efficient network traffic classification and advance the state of the art in deep learning and data mining techniques at scale, we explored techniques in data extraction and transformation of packets to uncover \textit{hidden representations} for learning. We then developed models designed for processing and classifying data based on these derived features. Our primary contribution in this dissertation is a systemization of these techniques and models into a network traffic classification toolkit ~\textsc{(\textbf{Forager})} which will be released and made available for public use.

\section{Problem Statement}
Network traffic classification covers a broad scope of problems; a limitation of existing research is that solutions tend to address only one or a subset of these issues without expanding to others. In this section, we introduce several of the most pressing problems to network traffic classification today and their relevance to real world systems.

\subsection{Scalability}

Machine learning and deep learning based systems are plagued by computational complexity and stream buffering requirements. In much of the existing research, entire traffic flows or certain portions of flows are required before classification can begin. In a system at scale processing terabits of data per second, it is not practical to buffer this many streams in dynamic memory. Furthermore, classification speeds and models must be capable of accelerated computation through hardware or simple enough to be run in parallel quickly to keep up with line rate. For real-time classification, it may not be possible to wait for multiple packets in a single stream to identify a particular flow. Even in offline forensic analysis, entire streams may not be available or recoverable. Rather, the system must make a best effort guess without knowing the state of the traffic as to what application layer protocol is being carried for immediate parsing and processing. If possible, classifying the application layer traffic per packet would provide the lowest latency and highest throughput possible in the system.

\subsection{Protocol Identification}

Protocol and data exchanges in the modern internet can be complex over the lifetime of the session exchange, particularly for large amounts of transferable data. Specifically, routines like Voice-over-IP (VoIP) calls, file transfers and downloads, streaming services, and peer-to-peer sharing can transmit hundreds of thousands of packets of variable data over several minutes to hours. These exchanges are also not guaranteed to be one-to-one; for example, content servers may be queried by hundreds of thousands of users at once for mass streaming services. Additionally, there is not a one-size-fits-all approach to protocol identification. Some protocols have strong signatures or commonalites; for example, HTTP requests tend to have methods like \texttt{GET} or \texttt{POST} present. SIP is similar, as it has methods like \texttt{INVITE} and \texttt{BYE}. Thus, it is easy and quick to search for these by keyword or regular expression. In contrast, the RTP data conveyed by VoIP is weakly signatured. Finally, for sessionization or reconstruction of whole data flows (for example, reconstructing a video call), multiple protocols such as SIP and RTP must be correctly identified and associated together in real time.

\subsection{Tunneling and Routing}

VPNs (virtual private networks) are private network infrastructures configured on top of public internet services. VPNs can be used for increased security, privacy, and anonimity. Furthermore, administrators may control how data is transmitted and what data can be accessed by whom according to the network configurations~\cite{Zhipeng2018VPNAB}. Depending on the site configuration, layers of tunneling, or dynamic mapping of networks, VPNs can add layers of IP encapsulation which obfuscates addresses. \textit{IP-over-IP encapsulation} means that the initial protocol stack of a stream can contain several layers of IP which may change over time. In intrusion detection systems, surveillance equipment, and other middlebox technologies as well as routing equipment, 3-tuple or 5-tuple information made up of source and destination IP addresses and port numbers are used to track streams. If IP layers are added or changed, this can interrupt stream tracking and cause unintended breaks or loss of streams. Attackers or covert operators may wish to avoid tracking by switching tunnels, as well, for these reasons~\cite{iscx-vpn-paper}. Without the ability to track content streams regardless of network layer changes, the data law enforcement and the intelligence community care about will become lost and untraceable.

Tor ("The Onion Routing") is an internet framework intended to combat fingerprinting to prevent third-party tracking and surveillance through multiple layers of encryption. The Tor browser can be freely downloaded and used by anyone to achieve virtual network anonymity and access to sites which may be otherwise blocked for certain users or regions~\cite{tor}. It accomplishes multiple layers of encryption through a series of network hops where each server provides unique encryption keys which must be wrapped on and then peeled off at each stop. Over two million people employ Tor services as of 2022~\cite{tor}; The vast majority of Tor usage is benign but studies show a significant portion of users do perform illegal activities on the network and disproportionately so in countries which have less internet regulation and government surveillance~\cite{Jardine2020ThePH}. Thus, it can be useful for surveillance and forensic operations to identify Tor traffic.

\subsection{Traffic Profiling}

Internet traffic can be characterized in a myriad of ways. For example, we previously introduced classifying by application layer protocol. One may wish to further classify traffic by application. It is a reasonable assumption that traffic generated by Skype will have commonalities versus traffic generated by Facebook Messenger. The application from which traffic originated can be useful information for law enforcement, for example to understand what tools suspects might be using.

Classification granularity could also be reduced to traffic type such as email or chat. This can be a more difficult problem as the features become more diverse across the individual classes. Still, several systems have successfully performed this traffic profiling even in encrypted and tunneled environments~\cite{iscx-tor-paper, iscx-vpn-paper, deeppacket, didarknet}. Many of these systems require entire traffic flows in order to perform this task, but we focus on single packet classification and other systems which follow the same paradigm.

\subsection{Encryption and Compression}

Traffic characteristics such as encryption, compression, and encoding can also be useful information for content inspection. Analysts may find plaintext traffic on port 443, for example, where they usually expect encrypted traffic. Being able to filter out and then further investigate this content could reveal important information related to surveillance or forensic operations. This also applies to filtering compressed traffic from encrypted traffic, as compression is easily reversible. Distinguishing these two types can be difficult on payload analysis alone as both methods produce highly entropic data, whereas plaintext data is much less entropic. Finally, determining encryption type and compression type are both required steps to completing any decompression or decryption task. Thus, network traffic classification requires systems capable of performing encryption and compression detection on large traffic streams.

\subsection{Port Obfuscation and Spoofing}

In order to bypass firewalls or avoid detection, individuals may choose to send traffic over non-standard ports or attempt to spoof traffic as a particular content type by sending it over a known port. There are many reasons to send non SSL/TLS-encrypted traffic over port 443. One primary motivation is that most firewalls default-allow traffic on ports 443 and 80. Thus, traffic may pass through alongside other data without the need for additional configuration or permissions. Devices which may wish to configure with a local network with minimal user or administrator overhead, such as Internet of Things (IoT) devices, have been discovered to send cleartext traffic alongside the encrypted traffic on these ports~\cite{wood2017cleartext}. Where security is not regulated or enforced, encryption standards also may just be ignored for simplicity's sake. Because the traffic is typically encrypted, payload-based deep packet inspection is often left unemployed on port 443 in favor of using compute resources on other traffic channels. Thus, port 443 has the potential to be used as a covert data channel if left unscanned. This has been realized in the real world; research has uncovered applications of foreign origin on public university networks running traffic through port 443 in order to avoid firewall detection~\cite{alcock2016sneaking}. Threat actors may route data intended for other commonly scanned ports, such as email traffic and file transfer data, through 443 to avoid content-based inspection~\cite{paya2009inspecting, lifliand2013encrypted, zander2005automated}. Tunneling protocols like SSH may also be run using HTTPS to create encrypted covert data channels, as well.
\subsection{Fingerprinting}

As previously mentioned, IP-over-IP encapsulation and other dynamic network configurations can cause problems for tuple-based stream tracking which is often used in routing and surveillance equipment. When this network layer information changes, middlebox technologies must be capable of resilient adaptation to find and continue profiling particular user streams.

\section{Proposed Solution}

Abstractly, the process of network traffic classification can be broken down into steps: deep packet inspection, representation learning, and machine learning-based classification. The task of identifying or classifying network traffic can begin with shallow or deep packet inspection. In these steps encapsulation layer and payload information of the packet is analyzed for matching content or features indicative of a particular class. This data may be further expanded through forms of representation learning or data transformation, where hidden features may be uncovered and used as input to the next step, machine learning. These algorithms such as neural networks or state vector machines learn these representations as classes and use the embedded data for the future testing phase in order to appropriately group incoming data. At runtime, a result for classification is returned on the input data. In this work, we provide novel contribution in applying several new forms of representation learning through data mining and transformation steps, as  well as appropriate deep learning classifiers following the creation of the hidden features.

This work proposes \textsc{Forager} as a toolbox for network traffic classification using data mining and deep learning methods. It is a culmination of lessons learned through the development of these modules and appropriately addresses the problems discussed in the scope of this dissertation. For scalability, \textsc{Forager} uses single-packet inspection techniques which may also be run in parallel for ultimate optimization. We demonstrate high success and negligible performance impact in protocol autodetection, traffic type and application profiling, and user fingerprinting. The system is also able to adapt to port obfuscation, spoofing, and tunneling architecture and identify VPN and Tor routing. Other types of data information such as compression, compression type, and encryption are able to be profiled through the \textsc{Forager} models.

\section{Summarization}

In the following chapters, we first explain the background work behind deep packet inspection and the path toward deep-learning based traffic classification. Chapter 3 introduces our work into automatic regular expression signature generation (\textsc{RExACtor}), which learns commonalities between packet payloads, transforms them into regular expressions, and performs text-based automata searching of the data. In chapter 4, systems based on locality-sensitive hashing (\textsc{Alpine} and \textsc{Palm}) introduce data compression and fixed storage space along with a decision-tree based forest classifier. We also introduce multimodality, a recurring enhancement to classification in our system. chapters 5 and 6 introduce \textsc{Maple} and \textsc{Date} respectively, which perform two-dimensional and three-dimensional transformations of data. In the \textsc{Maple} architecture, we further apply CNN classification. In \textsc{Date}, density-based clustering analysis is performed and statistics used as input to a neural network classifier. Finally, the ensemble system introduced in chapter 7 is the culmination of the previous models and the basis for \textsc{Forager} as a useable application for real cyber engineers and surveillance specialists. In chapter 8 of this work, we present a timeline for \textsc{Forager} development and completion. Chapter 9 concludes with further research opportunities and a summarization of issues discussed.
