One of the most critical cybersecurity interfaces is the connection to the worldwide Internet. Over a petabyte of data is sent over the Internet every second. The majority of traffic is broken down into transportable, encapsulated units of data called packets. These packets contain multiple layers of transport and infrastructure information in headers which wrapper application and user data known as the payload. These layers are defined by Internet protocols, which are standardized by the Internet community in technical documentation. While payload data was originally assumed to be benign, malicious software downloads, viruses, and spyware are just a few examples of what may be hidden inside packet payloads. Attackers may also be trying to exfiltrate non-malicious but sensitive information such as enterprise or government secrets, national intelligence, or personally identifiable information. Futhermore, it is not just attackers surveying this data, as lawful interception must be performed for Internet regulation, network administration, counter-cyberterrorism, and criminal investigation.


In order to use deep learning and data mining techniques to perform network traffic classification, features must be extracted from packets to use as input and define classes. Techniques such as dynamic port allocation, encryption and compression of payloads, spoofing, and tunneling have significant impact on the ability of systems to extract meaningful features from packets. Furthermore, these features can be of high dimensionality or heterogenous and must be normalized and reduced to a processable, comparable format for machine learning algorithms. In this work, we explore techniques in data extraction and transformation of packets to uncover \textit{hidden representations} for learning. We then develop and optimize models for processing and classifying data based on these derived features. The primary contribution of this dissertation is the systemization of these techniques and models into a network traffic classification toolkit~\textsc{(\textbf{Forager}~\footnote{https://pypi.org/project/forager-toolkit/})} that has been released and made available for public use.

\section{Problem Statement}

The unsolved challenges in deep packet inspection that we address with \textsc{Forager} can be categorized into three sections: data complexity, network complexity, and scalability.

\subsection{Data Complexity}

Packet classification is not one-dimensional; the data is \textit{complex} and \textit{multi-faceted}. An individual packet may be accurately classified in multiple ways. For example, a packet may be sent by user Bob containing email data characterized by the POP3 application-layer protocol. Depending on the intent of the classifier, the same packet may be classified as email traffic, encrypted or compressed, benign traffic, traffic belonging to user Bob, or traffic containing POP3 data. Classification granularity could also be reduced to traffic type such as email or chat. This can be a more difficult problem as the features become more diverse across the individual classes. One limitation of existing research is that solutions tend to address only one classification problem or a subset of issues without recognizing or expanding to others. In contrast, we show in subsequent chapters how \textsc{Forager} considers the same datasets in multiple ways and can generalize to new and interesting queries.

An additional layer of complexity to packet data itself is modern encryption and compression of payloads. In order to properly process the data, one must first know if data is encrypted, compressed, or plaintext. This is not always obvious through header information or by port configuration, as it is possible to obfuscate these for malicious purposes. Analysts may find plaintext traffic on port 443, for example, where they usually expect encrypted traffic. Similarly, they may be able to decompress traffic if they can distinguish it from similarly entropic encrypted traffic. Being able to filter out and then further investigate this content could reveal important information related to surveillance or forensic operations.

Finally, payloads or protocol standards themselves are not always strongly signatured. Particularly data protocols such as FTP (file transfer protocol) Data or RTP (realtime transfer protocol) can be highly entropic and therefore hard to identify even with state-of-the-art, regular expression scanning or by indicators in the packet header. \textsc{Forager} uses hidden representation learning to better understand and distinguish this data through generative features.

\subsection{Network Complexity}

Identifying traffic through a single packet can be difficult also because of network complexity. Protocol and data exchanges in the modern Internet are complex over the lifetime of the session exchange, particularly for large amounts of transferable data. Routines like Voice-over-IP (VoIP) calls, file transfers and downloads, streaming services, and peer-to-peer sharing can transmit hundreds of thousands of packets of highly variable data over several minutes to hours. These exchanges are also not guaranteed to be one-to-one; for example, content servers may be queried by hundreds of thousands of users at once for mass streaming services.

Deep packet inspection for the purpose of identifying traffic type or protocol is often performed for sessionization or reconstruction of whole data flows (for example, reconstructing a video call). Individual packets from multiple protocols such as SIP (session initiation protocol) and RTP must be correctly identified and associated together in real time to capture and correctly decode the full message.

Another complication for DPI is obfuscated network or routing information due to network architecture's advances in anonimity. VPNs (virtual private networks) are private network infrastructures configured on top of public Internet services. VPNs can be used for increased security, privacy, and anonimity. Furthermore, administrators may control how data is transmitted and what data can be accessed by whom according to the network configurations. Depending on the site configuration, layers of tunneling, or dynamic mapping of networks, VPNs can add layers of Internet protocol (IP) encapsulation which obfuscates addresses. \textit{IP-over-IP encapsulation} means that the initial protocol stack of a stream can contain several layers of IP which may change over time. In many intrusion detection systems, surveillance equipment, and other middlebox technologies, 3-tuple or 5-tuple information made up of source and destination addresses and port numbers are used to track streams. If layers are added or changed, this can interrupt stream tracking and cause unintended breaks or loss of streams. Attackers or covert operators may wish to avoid tracking by switching tunnels, as well. Without the ability to track content streams regardless of network layer changes, the data law enforcement and the intelligence community care about will become lost and untraceable.

Tor ("The Onion Routing")~\cite{tor} is an internet framework intended to combat fingerprinting to prevent third-party tracking and surveillance through multiple layers of encryption. The Tor browser can be freely downloaded and used by anyone to achieve virtual network anonymity and access to sites which may be otherwise blocked for certain users or regions. It accomplishes multiple layers of encryption through a series of network hops where each server provides unique encryption keys which must be wrapped on and then peeled off at each stop. Over two million people employ Tor services as of 2022; The vast majority of Tor usage is benign but studies show a significant portion of users do perform illegal activities on the network and disproportionately so in countries which have less Internet regulation and government surveillance~\cite{Jardine2020ThePH}. Thus, it can be useful for surveillance and forensic operations to identify Tor traffic and necessary for DPI systems employed to do so.

There are also active adversaries who work to confuse DPI and avoid tracking. In order to bypass firewalls or avoid detection, threat actors may choose to send traffic over non-standard ports or attempt to spoof traffic as a particular content type by sending it over a known port. There are many reasons to send non SSL/TLS-encrypted traffic over port 443. One primary motivation is that most firewalls default-allow traffic on ports 443 and 80. Thus, traffic may pass through alongside other data without the need for additional configuration or permissions. Devices which may wish to configure with a local network with minimal user or administrator overhead, such as Internet of Things (IoT) devices, have been discovered to send cleartext traffic alongside the encrypted traffic on these ports. Where security is not regulated or enforced, encryption standards also may just be ignored for simplicity's sake. Because the traffic is typically encrypted, payload-based deep packet inspection is often left unemployed on port 443 in favor of using compute resources on other traffic channels. Thus, port 443 has the potential to be used as a covert data channel if left unscanned. This has been realized in the wild; research has uncovered applications of foreign origin on public university networks running traffic through port 443 in order to avoid firewall detection. Threat actors may route data intended for other commonly scanned ports, such as email traffic and file transfer data, through 443 to avoid content-based inspection. Tunneling protocols like SSH may also be run using HTTPS to create encrypted covert data channels. In this work, we discuss this exact threat scenario and employ \textsc{Forager} to profile traffic on port 443, providing a clearer picture to nefarious activity which may be occurring.

\subsection{Scalability}

Machine learning and deep learning based systems are plagued by computational complexity and stream buffering requirements. In much of the existing research, entire traffic flows or certain portions of flows are required before classification can begin. In a system at scale processing terabits of data per second, it is not practical to buffer this many streams in dynamic memory. Furthermore, classification speeds and models must be capable of accelerated computation through hardware or simple enough to be run in parallel quickly to keep up with line rate. For real-time classification, it may not be possible to wait for multiple packets in a single stream to identify a particular flow. Even in offline forensic analysis, entire streams may not be available or recoverable. Rather, the system must make a best effort guess without knowing the state of the traffic as to what application layer protocol is being carried for immediate parsing and processing. If possible, classifying the application layer traffic per packet would provide the lowest latency and highest throughput possible in the system.

\section{Proposed Solution}

The process of network traffic classification can be broken down into steps: deep packet inspection, representation learning, and machine learning-based classification. The task of identifying or classifying network traffic can begin with shallow or deep packet inspection. In these steps encapsulation layer and payload information of the packet is analyzed for matching content or features indicative of a particular class. This data may be further expanded through forms of representation learning or data transformation, where hidden features may be uncovered and used as input to the next step, machine learning. These algorithms such as neural networks or state vector machines learn these representations as classes and use the embedded data for the future testing phase in order to appropriately group incoming data. At runtime, a result for classification is returned on the input data. In this work, we provide novel contribution in applying several new forms of representation learning through data mining and transformation steps, as  well as appropriate deep learning classifiers following the creation of the hidden features.

This work proposes \textsc{Forager} as a toolbox for network traffic classification using data mining and deep learning methods. It is a culmination of lessons learned through the development of these modules and appropriately addresses the problems discussed in the scope of this dissertation. For scalability, \textsc{Forager} uses single-packet inspection techniques which may also be run in parallel for ultimate optimization. We demonstrate high success and negligible performance impact in protocol autodetection, traffic type and application profiling, and user fingerprinting. The system is also able to adapt to port obfuscation, spoofing, and tunneling architecture and identify VPN and Tor routing. Other types of data information such as compression, compression type, and encryption are able to be profiled through the \textsc{Forager} models.

The scope of this work focuses on traditional network data. Specialized datasets such as Internet of Things networks, mobile networks, and vehicular networks are not considered here but would be future opportunities for expansion. We also intentionally do not consider traffic flows or flow-based features. The data inputs and classifications performed in this work are done on a per-packet basis for minimized latency and buffering. Hardware optimization and acceleration has been proposed in other literature but are outside this scope. The focus of this work is the software implementations of data engineering, transformation, and analysis. Scalability is assessed through runtime metrics such as testing speed, throughput of data through the system (transformation plus classification), and dynamic memory usage. The models are also guaged with performance metrics from machine learning such as precision, recall, and F1-score.

\section{Contributions}

As part of this dissertation, PCAP sources containing twenty-six different application layer protocols were processed by our \textbf{\textsc{Tapcap}} solution and combined into a diverse collection of tabularized data. We provide this dataset as a public resource for the community~\footnote{https://github.com/mayakapoor/protocol-dataset}. As a deliverable, we also publish \textsc{Forager 1.0}~\footnote{https://pypi.org/project/forager-toolkit/} along with support packages for \textsc{Rexactor 1.0}~\footnote{https://pypi.org/project/rexactor/} and \textsc{Tapcap 1.0}~\footnote{https://pypi.org/project/tapcap/}. The goal of providing these software packages and corresponding user documentation is to encourage the research community to use these tools in their own experiments in order to further apply what we have learned in work accomplished in this research. Each of the data engineering tools and deep learning tools provide intellectual contribution to the field in the following ways.

\textbf{\textsc{Rexactor}} is our automatic regular expression generation solution, furthering the state-of-the-art by encoding substrings through global sequence alignment and mining frequent tokens in an original algorithm for more enriched, expressive regular expressions than previous work. \textbf{\textsc{Palm}} (\underline{P}ayload \underline{A}nalysis using \underline{L}ocality-Sensitive \underline{M}easurements) and \textbf{\textsc{Alpine}} (\underline{A} \underline{L}ocality-Sensitive \underline{P}acket \underline{IN}spection \underline{E}ngine) are innovative methods for creating locality-sensitive hash embeddings from network traffic which is a unique alternative to regular expression scanning previously used for deep packet inspection. Compared to the previous method, the tree-based search scales sublinearly and the fixed hash representations require a linear amount of storage space. These methods accurately identify many classes such as traffic type, protocol type, application, and more. Our \underline{MA}trix \underline{P}ay\underline{L}oad \underline{E}ncoder, \textbf{\textsc{Maple}}, and \underline{D}ensity-based \underline{A}nalysis \underline{T}ensor \underline{E}ncoder, \textbf{\textsc{Date}}, explore applications of image-based and density analysis-based embeddings to an unsolved problem of RTP detection and profiling data protocols. The \textsc{Date} model is a unique approach to three-dimensional, point cloud modeling of packets which is not covered in previous literature and proves to have applications in our experiments.

Each model is assessed with measures of precision, recall, and F1-score in different classification problems. We also provide comparison between models in order to better understand which transformations work better for different network environments and different traffic types or problems. In several of the experiments we also compare against state-of-the-art work such as the Hyperscan regular expression scanning library or other machine learning profiling techniques and systems to show our system contribution. Our system provides an innovation also in its multi-modality, which we demonstrate in a case study where we analyze traffic in multiple ways on port 443. For scalability, we also assess memory usage and throughput across scenarios to propose the system at scale so that users may be able to design their classification strategy to their own requirements and available resources.

\section{Summarization}

In the following chapters, we first explain the background work behind deep packet inspection and the path toward deep-learning based traffic classification. Chapter 3 introduces our work into automatic regular expression signature generation (\textsc{Rexactor}), which learns commonalities between packet payloads, transforms them into regular expressions, and performs text-based automata searching of the data. In chapter 4, systems based on locality-sensitive hashing (\textsc{Alpine} and \textsc{Palm}) introduce data compression and fixed storage space along with a decision-tree based forest classifier. We also introduce multimodality, a recurring enhancement to classification in our system. chapters 5 and 6 introduce \textsc{Maple} and \textsc{Date} respectively, which perform two-dimensional and three-dimensional transformations of data. In the \textsc{Maple} architecture, we further apply CNN classification. In \textsc{Date}, density-based clustering analysis is performed and statistics used as input to a neural network classifier. Finally, the ensemble system introduced in chapter 7 is the culmination of the previous models and the basis for \textsc{Forager} as a useable application for real cyber engineers and surveillance specialists. In chapter 8 of this work, we present the \textsc{Forager} software and documentation resources. Chapter 9 concludes with further research opportunities and a summarization of issues discussed.
